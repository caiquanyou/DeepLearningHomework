# <center>自动写诗实验报告</center>

## <center>摘要</center>

本次实验采用深度学习中的循环神经网络来训练得到一个自动写诗的程序，在实验中掌握文本的数据读取、循环神经网络设计构建（实验中采用LSTM模型进行训练）、模型训练和模型测试等过程，最终实现一个给出首句，根据首句可以自动续写后续诗句的程序，并通过调整参数进行优化。

### 问题描述

实验希望可以通过人工给出首字或者首句，自动输出后续顺畅的诗句；实验需要对数据集进行文本数据操作读取，训练基于循环神经网络的深度学习模型，在自动续写的诗句中要符合句子通顺。

### 解决方案

利用pytorch框架构建一个循环神经网络组织结构；同样使用 pytorch的 DataLoader进行数据读取，在网络中输入的字词序号经过词向量层nn.Embedding得到相应的词向量表示，然后利用设计好的LSTM提取词的所有隐藏层的信息，再将信息进行分类，判断输出属于每一个词的概率。

#### 损失函数设计

在这次实验中仍然使用交差熵作为损失函数，交叉熵定义如下：
$$
H_y(\hat y)=-\sum_i^n ylog\hat y
$$
其中y代表真实值，$\hat y$代表预测值，n在这里表示词库长度

#### 网络结构设计

1. 词向量层：设置好词长度len(ix2word)和嵌入维度为256;
2. 3层的LSTM网络：输入为256个节点，输出为隐层512个节点；
3. 全连接层1：输出为512个神经元，输出为256个神经元；
4. 全连接层2：输入为256个神经元，输出为len(ix2word)。

#### 数据集介绍

tang.npz数据集中里面包含三个对象：

data：为（57580,125）的numpy数组，总共包含57580首诗歌，每首诗歌长度为125个字符（不足125补空格/s，超过125的丢弃）；
word2ix：每个词和它对应的序号；
ix2word：每个序号和它对应的词。

#### 实验结果与分析

迭代50次运行数据结果如下：

```
第1轮迭代训练损失为2.8626604458093645
第2轮迭代训练损失为2.6359081457853284
第3轮迭代训练损失为2.5474761972824744
第4轮迭代训练损失为2.4850157676140423
第5轮迭代训练损失为2.4362885322173455
第6轮迭代训练损失为2.388490895271304
第7轮迭代训练损失为2.3398059153159485
第8轮迭代训练损失为2.3044661358197454
第9轮迭代训练损失为2.255088159163794
第10轮迭代训练损失为2.2243144385019957
第11轮迭代训练损失为2.1894694830973944
第12轮迭代训练损失为2.1624174213806806
第13轮迭代训练损失为2.129087828119596
第14轮迭代训练损失为2.110904317379002
第15轮迭代训练损失为2.0924297076066423
第16轮迭代训练损失为2.071372367064163
第17轮迭代训练损失为2.057199134985616
第18轮迭代训练损失为2.0500077357689586
第19轮迭代训练损失为2.0306269498268716
第20轮迭代训练损失为2.0250816448132207
第21轮迭代训练损失为2.0093941479126665
第22轮迭代训练损失为1.997407113234201
第23轮迭代训练损失为1.994394109686217
第24轮迭代训练损失为1.985965568502742
第25轮迭代训练损失为1.9780418179035182
第26轮迭代训练损失为1.9721597023804966
第27轮迭代训练损失为1.966645176251731
第28轮迭代训练损失为1.9650738881429064
第29轮迭代训练损失为1.9571165649493527
第30轮迭代训练损失为1.9550100022157055
第31轮迭代训练损失为1.9502372124989829
第32轮迭代训练损失为1.9520250931183474
第33轮迭代训练损失为1.9459148702621443
第34轮迭代训练损失为1.9448958883682872
第35轮迭代训练损失为1.936857673009233
第36轮迭代训练损失为1.9321170001029968
第37轮迭代训练损失为1.9320819177627582
第38轮迭代训练损失为1.9291814735333106
第39轮迭代训练损失为1.92129502395789
第40轮迭代训练损失为1.917340785821281
第41轮迭代训练损失为1.9143247937361425
第42轮迭代训练损失为1.9142651270230606
第43轮迭代训练损失为1.9109769101540257
第44轮迭代训练损失为1.9050804243087776
第45轮迭代训练损失为1.9154291810989392
第46轮迭代训练损失为1.9188678598006566
第47轮迭代训练损失为1.9148581767082187
第48轮迭代训练损失为1.9049196092287701
第49轮迭代训练损失为1.9038325940767928
第50轮迭代训练损失为1.8993609906037674
```

实验中模型第一次采用指导书中的单层LSTM网络结构进行测试，在训练集上训练损失较高，输出的诗句有不连续的，且有时候有长有短；调整改进网络结构为三层LSTM后训练损失显著下降，实验的时候尝试添加了dropout层，改进效果不明显，添加了多一层全连接层后运行时间增加了，训练损失稍微有减少；

测试结果如下：

```
加载数据集
成功加载唐诗数据集

请输入诗歌开头
千树万树梨花开
生成的诗句如下：千树万树梨花开，秋风吹落碧莲城。海鸟疑凤不相问，何处山川飞远林。今年经春又万树，草草寂沦生一枝。此中幽人谁识我，云山一片若风吹。
请输入诗歌开头
深度学习
生成的诗句如下：深度学习真，心随静道流。经时不在道，外事无人忧。天子方在隐，归来未相闻。今我何功雅，居此为余云。古今相传密，今日一超幽。
请输入诗歌开头
床前明月光
生成的诗句如下：床前明月光，天地何一重。水榭长有声，公门有余行。出门得所见，所遇多知心。此情不复迷，所遇唯高情。极居无白日，紫塞方盛成。

```

可以看到输出的诗句基本通顺，也出现一些比较押韵，虽然有一部分诗句仍然不通顺，但是基本输出诗句可以符合语法规范。

### 总结

这次实验巩固了学习到的循环神经网络的内容，实验中采用了LSTM的神经网络结构，对比了指导书中给的基础网络结构做出了小改动，训练损失得到下降，虽然有一部分诗句仍然不通顺，但是偶尔会出现一些比较押韵的诗句，基本输出诗句也都符合语法规范，实验的时候尝试添加了dropout层，改进效果不明显，添加了多一层全连接层后运行时间增加了，训练损失稍微有减少；输入诗句开头如果是古诗原句的出来的续写诗句更加通顺一些，如果输入的开头是一些外来词或者是人名则紧跟着的续写的诗句容易出现语法不通，续写长一些之后才恢复。
